#
# Arguments:1. data: In this case we input the simulation data
#
#
# Retutn: 1.final: which is the best lambda,in each case of making X_i as response, there will be a lambda that minimised the test error.
#                  In this case, the best lambda 'final' is the mean of these lambda. Further discussion needed here.
choose_best_lambda <- function(data){
numOfRows <- nrow(data)
numOfDims <- ncol(data)
lambda.best_list = rep(0,numOfDims)
for (i in seq(numOfDims)){
y <-(testdata[,i])
x <- (testdata[,-c(i)])
train <-sample(seq(numOfRows), 0.6*numOfRows, replace=FALSE)
lasso.train <-glmnet(x[train,], y[train])
pred.test <-predict(lasso.train, x[-train,])
rmse <-sqrt(apply((y[-train]-pred.test)^2,2,mean))
lambda.best <-lasso.train$lambda[order(rmse)[1]]
lambda.best_list[i] <- lambda.best
}
final <- mean(lambda.best_list)
return(final)
}
# Lin:
# This is the code for simulating multivariate gaussian distribution with zero mean and the covariance matrix sigma = theta^-1.
# Be careful about 'the choice of delta'. Will discuss about the official way to choose delta in the future days.
#
# Arguments:1. number_of_dimensions: the number of variables/features
#           2. how_many_sets_of_data_you_need: As the name said, the number of rows of data do you want to simulate?
#
# Retutn: 1.testdata: the simulated data in a matrix.
#
simulation <- function(number_of_dimensions, how_many_sets_of_data_you_need){
library(MASS)
library(matrixcalc)
#number of dimension
nod <- number_of_dimensions
#a generate the lower triangle part of the nodxnod matrix with 10% to be 0.5 and 90% to be 0.
a <- rbinom(n = nod * (nod - 1) / 2 , size = 1, prob = 0.1)
a[a == 1] <- 0.5
#B become the B in the sheet. Diagnal is all 0.
B <- matrix(0, nod, nod)
B[lower.tri(B, diag = FALSE)] <- a
B[upper.tri(B)] <- t(B)[upper.tri(B)]
#indentity matrix
I <- diag(x = 1, nod, nod)
#delta is something I am pretty not sure about. I am confused with the word 'chosen' used in the project
#guidance paper so I just simply sample one number from 1 to 100 which can make the theta postive
#definite.
delta <- 0
is.positive.definite(B+delta*I, tol=0)
while (is.positive.definite(B+delta*I, tol=0)==FALSE){delta <- delta + 1}
#theta
theta = B + delta*I
#standardize the theta
standard_theta <- cov2cor(theta)
#calculate the inverse of theta
covMatrix <- solve(standard_theta)
#generate nod random samples from a multivariate gaussian distribution with zero mean and the covariance matrix sigma = theta^-1.
testdata <- mvrnorm(n = how_many_sets_of_data_you_need, mu = numeric(nod), Sigma = covMatrix, tol = 0, empirical = FALSE, EISPACK = FALSE)
ls1 <-  list("data" = testdata, "standardtheta" = standard_theta, "theta" = theta)
return(ls1)
}
testdata <- simulation(10,100)$data
View(testdata)
final <- choose_best_lambda(testdata)
Edge_Table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
colnames(edge) <- seq(numOfDims)
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(i)]
coeff <- append(coeff, 1, after= i-1)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
return(tf)
}
edge_Table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
colnames(edge) <- seq(numOfDims)
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(i)]
coeff <- append(coeff, 1, after= i-1)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
return(tf)
}
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
colnames(edge) <- seq(numOfDims)
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(i)]
coeff <- append(coeff, 1, after= i-1)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
return(tf)
}
edge_table(testdata, final)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(i)]
coeff <- append(coeff, 1, after= i-1)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
edge_table(testdata, final)
x <- simulation(5,100)
x$standardtheta
x$theta
x <- simulation(10,100)
x <- simulation(10,100)
x$standardtheta
testdata <- x$data
final <- choose_best_lambda(testdata)
edge_table(testdata, final)
x <- simulation(10,100)
testdata <- x$data
testdata <- x$data
final <- choose_best_lambda(testdata)
edge_table(testdata, final)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(i)]
coeff <- append(coeff, 1, after= i-1)
edge[i,] <- coeff
}
print(edge)
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
edge_table(testdata, final)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(i)]
coeff <- append(coeff, 1, after= i-1)
print(coeff)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
edge_table(testdata, final)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
pritn(coeff)
coeff <- as.matrix(coeff)[-c(i)]
coeff <- append(coeff, 1, after= i-1)
#print(coeff)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
edge_table(testdata, final)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
print(coeff)
coeff <- as.matrix(coeff)[-c(i)]
coeff <- append(coeff, 1, after= i-1)
#print(coeff)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
edge_table(testdata, final)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
print(coeff)
coeff <- as.matrix(coeff)[-c(i)]
print(coeff)
coeff <- append(coeff, 1, after= i-1)
#print(coeff)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
a
edge_table(testdata, final)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
print(coeff)
coeff <- as.matrix(coeff)[-c(1)]
print(coeff)
coeff <- append(coeff, 1, after= i-1)
#print(coeff)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
print(coeff)
coeff <- as.matrix(coeff)[-c(1)]
print(coeff)
coeff <- append(coeff, 1, after= i-1)
#print(coeff)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
edge_table(testdata, final)
knitr::opts_chunk$set(echo = TRUE)
airbnb = read.csv("st445_final_data.csv", header = T)
knitr::opts_chunk$set(echo = TRUE)
airbnb = read.csv("st445_final_data.csv", header = T)
airbnb = read.csv("st445_final_data", header = T)
airbnb3 = subset(airbnb, select = -c(1,17))
attach(airbnb)
str(airbnb)
regn = lm(realprice ~., airbnb3)
summary(regn)
library(MASS)
boxcox(regn)
#split the data into training and testing dataset
airbnb1 = subset(airbnb, select = -c(1,15))
traingsize = floor(0.8*nrow(airbnb1))
set.seed(123)
train_ind = sample(seq_len(nrow(airbnb1)),size = traingsize)
train=airbnb1[train_ind,]
test=airbnb1[-train_ind,]
attach(airbnb)
str(airbnb)
reg1 = lm(logprice ~., train)
summary(reg1)
library(leaps)
install.packages(leaps)
install.packages("leaps")
knitr::opts_chunk$set(echo = TRUE)
#split the data into training and testing dataset
airbnb1 = subset(airbnb, select = -c(1,15))
traingsize = floor(0.8*nrow(airbnb1))
set.seed(123)
train_ind = sample(seq_len(nrow(airbnb1)),size = traingsize)
train=airbnb1[train_ind,]
test=airbnb1[-train_ind,]
attach(airbnb)
str(airbnb)
reg1 = lm(logprice ~., train)
summary(reg1)
library(leaps)
reg2=regsubsets(logprice~.,nvmax = 20,data = train)
plot(reg2, scale = "adjr2")
summary(reg2)
plot(reg2, scale = "bic")
outbs=summary(reg2)
which.max(outbs$adjr2)
which.min(outbs$bic)
#check multicollinearity
library(car)
install.packages("car")
#split the data into training and testing dataset
airbnb1 = subset(airbnb, select = -c(1,15))
traingsize = floor(0.8*nrow(airbnb1))
set.seed(123)
train_ind = sample(seq_len(nrow(airbnb1)),size = traingsize)
train=airbnb1[train_ind,]
test=airbnb1[-train_ind,]
attach(airbnb)
str(airbnb)
reg1 = lm(logprice ~., train)
summary(reg1)
library(leaps)
reg2=regsubsets(logprice~.,nvmax = 20,data = train)
plot(reg2, scale = "adjr2")
summary(reg2)
plot(reg2, scale = "bic")
outbs=summary(reg2)
which.max(outbs$adjr2)
which.min(outbs$bic)
#check multicollinearity
library(car)
# Lin:
# This is the code for choosing best lambda based on the simulation data
#
# Arguments:1. data: In this case we input the simulation data
#
#
# Retutn: 1.final: which is the best lambda,in each case of making X_i as response, there will be a lambda that minimised the test error.
#                  In this case, the best lambda 'final' is the mean of these lambda. Further discussion needed here.
choose_best_lambda <- function(data){
numOfRows <- nrow(data)
numOfDims <- ncol(data)
lambda.best_list = rep(0,numOfDims)
for (i in seq(numOfDims)){
y <-(testdata[,i])
x <- (testdata[,-c(i)])
train <-sample(seq(numOfRows), 0.6*numOfRows, replace=FALSE)
lasso.train <-glmnet(x[train,], y[train])
pred.test <-predict(lasso.train, x[-train,])
rmse <-sqrt(apply((y[-train]-pred.test)^2,2,mean))
lambda.best <-lasso.train$lambda[order(rmse)[1]]
lambda.best_list[i] <- lambda.best
}
final <- mean(lambda.best_list)
return(final)
}
# Lin:
# This is the code for simulating multivariate gaussian distribution with zero mean and the covariance matrix sigma = theta^-1.
# Be careful about 'the choice of delta'. Will discuss about the official way to choose delta in the future days.
#
# Arguments:1. number_of_dimensions: the number of variables/features
#           2. how_many_sets_of_data_you_need: As the name said, the number of rows of data do you want to simulate?
#
# Retutn: 1.testdata: the simulated data in a matrix.
#
simulation <- function(number_of_dimensions, how_many_sets_of_data_you_need){
library(MASS)
library(matrixcalc)
#number of dimension
nod <- number_of_dimensions
#a generate the lower triangle part of the nodxnod matrix with 10% to be 0.5 and 90% to be 0.
a <- rbinom(n = nod * (nod - 1) / 2 , size = 1, prob = 0.1)
a[a == 1] <- 0.5
#B become the B in the sheet. Diagnal is all 0.
B <- matrix(0, nod, nod)
B[lower.tri(B, diag = FALSE)] <- a
B[upper.tri(B)] <- t(B)[upper.tri(B)]
#indentity matrix
I <- diag(x = 1, nod, nod)
#delta is something I am pretty not sure about. I am confused with the word 'chosen' used in the project
#guidance paper so I just simply sample one number from 1 to 100 which can make the theta postive
#definite.
delta <- 0
is.positive.definite(B+delta*I, tol=0)
while (is.positive.definite(B+delta*I, tol=0)==FALSE){delta <- delta + 1}
#theta
theta = B + delta*I
#standardize the theta
standard_theta <- cov2cor(theta)
#calculate the inverse of theta
covMatrix <- solve(standard_theta)
#generate nod random samples from a multivariate gaussian distribution with zero mean and the covariance matrix sigma = theta^-1.
testdata <- mvrnorm(n = how_many_sets_of_data_you_need, mu = numeric(nod), Sigma = covMatrix, tol = 0, empirical = FALSE, EISPACK = FALSE)
ls1 <-  list("data" = testdata, "standardtheta" = standard_theta, "theta" = theta)
return(ls1)
}
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(1)]
coeff <- append(coeff, 1, after= i-1)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
x$standardtheta
testdata <- x$data
final <- choose_best_lambda(testdata)
edge_table(testdata, final)
x <- simulation(10,100)
testdata <- x$data
final <- choose_best_lambda(testdata)
edge_table(testdata, final)
x$standardtheta
true_edge <- function(theta){
tf <- data.frame(lapply(theta, function(x) {x!=0}))
return(tf)
}
true_edge(x$standardtheta)
a <- true_edge(x$standardtheta)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(1)]
coeff <- append(coeff, 1, after= i-1)
edge[i,] <- coeff
}
print(edge)
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
edge_table(testdata, final)
x$standardtheta
true_edge <- function(theta){
theta <- data.frame(theta)
tf <- data.frame(lapply(theta, function(x) {x!=0}))
return(tf)
}
a <- true_edge(x$standardtheta)
View(a)
true_edge(x$standardtheta)
x <- simulation(3,100)
x$standardtheta
x$theta
x <- simulation(3,100)
x$standardtheta
x <- simulation(5,100)
x$standardtheta
x <- simulation(5,100)
x$standardtheta
x$theta
test<-x$data
testdata<-x$data
final <- choose_best_lambda(testdata)
edge_table(testdata, final)
pred <- edge_table(testdata, final)
edge_table <- function(data_set, lambda_choice){
numOfRows <- nrow(data_set)
numOfDims <- ncol(data_set)
edge <- data.frame(matrix(ncol = numOfDims, nrow = numOfDims))
for (i in seq(numOfDims)){
y <- (data_set[,i])
x <- (data_set[,-c(i)])
coeff <- coef(glmnet(x,y, lambda=lambda_choice))
coeff <- as.matrix(coeff)[-c(1)]
coeff <- append(coeff, 1, after= i-1)
edge[i,] <- coeff
}
tf <- data.frame(lapply(edge, function(x) {x!=0}))
colnames(tf) <- seq(numOfDims)
return(tf)
}
pred <- edge_table(testdata, final)
View(pred)
true <- true_edge(x$theta)
confusionMatrix(pred,true,postive = TRUE)
confusionMatrix(pred,true,postive = TRUE)
install.packages("caret")
library(caret)
confusionMatrix(pred,true,postive = TRUE)
