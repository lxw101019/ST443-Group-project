---
title: "LASSO and Ridge Regression amsterdam"
author: "Laurens van der Maas"
date: "12/1/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Shrinkage Methods
In this section shrinkage methods are applied to the AirBnB data. These methods constrain the coefficient estimates, shrinking them towards zero. This approach can significantly reduce the variance. 

## Ridge Regression
In a ridge regression, instead of using the least squares fitting procedure typical of OLS a penalty term is included in the estimation of the coefficients. The parameter $\lambda$ determines the size of this penalty. The larger the $\lambda$ the more the oefficients are shrunk. 

In terms of fractional deviance explained, it turns out that the (dummy) variables Private Room, Shared Room, super_strict_30, super_strict_60, near_centre are most important. Some of these, however, converge to zero faster than other variables that explain less of the fractional deviance. 

In the optimal setting, the log of lambda is near -4. The optimal lambda is equal to 0.022 and the MSE on the test set is equal to 0.380. This optimal lambda is very small and therefore the model is not very different from the MLR model. Even more, the test MSE of the ridge regression with a lambda optimal for the training set is larger than that of the MLR. This method is therefore not relevant on this dataset.

```{r cars, warning=FALSE, message=FALSE}
suppressMessages(library(readr))
suppressMessages(library(glmnet))

amsterdam <- read_csv('st443_final_data')
amsterdam <- amsterdam[,-c(1,15)]

# glmnet does not use formula language
x <- model.matrix(logprice ~ ., data = amsterdam)
y <- amsterdam$logprice

fit.ridge <-glmnet(x, y, alpha=0)

# 8, 7, 15, 20, 16 most important vars
plot(fit.ridge, xvar="lambda", label= TRUE)
plot(fit.ridge, xvar="dev", label= TRUE)

cv.ridge <-cv.glmnet(x, y, alpha=0)

## Plot of CV mse vs log (lambda), small lambda is best
plot(cv.ridge)
## Coefficent vector corresponding to the mse which is within 
# one standard error of the lowest mse using the best lambda.
coef(cv.ridge)

## Coefficient vector corresponding to the lowest mse using the best lambda
coef(glmnet(x,y,alpha=0, lambda=cv.ridge$lambda.min))

# finding MSE
traingsize = floor(0.7*nrow(amsterdam))
set.seed(123)
train = sample(seq_len(nrow(amsterdam)),size = traingsize)

ridge.train <-glmnet(x[train,], y[train], alpha = 0)
pred.test.ridge <-predict(ridge.train, x[-train,])
dim(pred.test.ridge)
rmse.ridge <-sqrt(apply((y[-train]-pred.test.ridge)^2,2,mean))
plot(log(ridge.train$lambda), rmse.ridge, type="b", xlab="Log(lambda)")
lambda.best.ridge <- ridge.train$lambda[order(rmse.ridge)[1]]
lambda.best.ridge
mseRidge <- min(rmse.ridge)
mseRidge
```

## LASSO Regression
A lasso regression is similar to a ridge regression in that it is a shrinkage method, but uses a different type of penalty term. In this type of model, some coefficient are shrunk to 0. The same five variables explain the largest part of the fractional deviance. 

The test MSE of the optimal LASSO regression is again very similar to that of the MLR. This method is therefore neither relevant on this dataset. The log lambda at optimum is near -8. Lambda equals 0.00036 and the test MSE equals 0.379. Again, this lambda is too low to have any significant impact on the estimation and none of the coefficients are shrunk to 0 at this lambda.

```{r}
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar="lambda", label= TRUE)
plot(fit.lasso, xvar="dev", label= TRUE)
cv.lasso <-cv.glmnet(x, y)
# Again, 8, 15, 7, 20. 

plot(cv.lasso)

# Use very small lambda, again
## coefficent vector corresponding to the mse which is within 
# one standard error of the lowest mse using the best lambda.
coef(cv.lasso)
## coefficient vector corresponding to the lowest mse using the best lambda
coef(glmnet(x,y, lambda=cv.lasso$lambda.min))

## test MSE
lasso.train <-glmnet(x[train,], y[train])
pred.test <-predict(lasso.train, x[-train,])
dim(pred.test)
rmse <-sqrt(apply((y[-train]-pred.test)^2,2,mean))
plot(log(lasso.train$lambda), rmse, type="b", xlab="Log(lambda)")
lambda.best <-lasso.train$lambda[order(rmse)[1]]
lambda.best
mseLasso <- min(rmse)
mseLasso
```


