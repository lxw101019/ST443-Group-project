covMatrix <- solve(standard_theta)
#generate nod random samples from a multivariate gaussian distribution with zero mean and the covariance matrix sigma = theta^-1.
testdata <- mvrnorm(n = n, mu = numeric(nod), Sigma = covMatrix, tol = 0, empirical = FALSE, EISPACK = FALSE)
newList <- list("theta" = standard_theta, "testdata" = testdata)
return(newList)
}
sampleData <- simulation(p = 5, n = 5000)
simulation <- function(p, n){
nod <- p
#a generate the lower triangle part of the nodxnod matrix with 10% to be 0.5 and 90% to be 0.
a <- rbinom(n = nod * (nod - 1) / 2 , size = 1, prob = 0.1)
a[a == 1] <- 0.5
#B become the B in the sheet. Diagnal is all 0.
B <- matrix(0, nod, nod)
B[lower.tri(B, diag = FALSE)] <- a
B[upper.tri(B)] <- t(B)[upper.tri(B)]
#indentity matrix
I <- diag(x = 1, nod, nod)
#delta is something I am pretty not sure about. I am confused with the word 'chosen' used in the project
#guidance paper so I just simply sample one number from 1 to 100 which can make the theta postive
#definite.
delta <- 2
#theta
theta = B + delta*I
print(is.positive.definite(theta))
print(theta)
#standardize the theta
standard_theta <- cov2cor(theta)
print(standard_theta)
#calculate the inverse of theta
covMatrix <- solve(standard_theta)
#generate nod random samples from a multivariate gaussian distribution with zero mean and the covariance matrix sigma = theta^-1.
testdata <- mvrnorm(n = n, mu = numeric(nod), Sigma = covMatrix, tol = 0, empirical = FALSE, EISPACK = FALSE)
newList <- list("theta" = standard_theta, "testdata" = testdata)
return(newList)
}
sampleData <- simulation(p = 5, n = 5000)
trueTheta <- randomSample$theta
trueTheta
trueTheta <- sampleData$theta
trueTheta
simulation <- function(p, n){
nod <- p
#a generate the lower triangle part of the nodxnod matrix with 10% to be 0.5 and 90% to be 0.
a <- rbinom(n = nod * (nod - 1) / 2 , size = 1, prob = 0.1)
a[a == 1] <- 0.5
#B become the B in the sheet. Diagnal is all 0.
B <- matrix(0, nod, nod)
B[lower.tri(B, diag = FALSE)] <- a
B[upper.tri(B)] <- t(B)[upper.tri(B)]
#indentity matrix
I <- diag(x = 1, nod, nod)
#delta is something I am pretty not sure about. I am confused with the word 'chosen' used in the project
#guidance paper so I just simply sample one number from 1 to 100 which can make the theta postive
#definite.
delta <- 2
#theta
theta = B + delta*I
print(is.positive.definite(theta))
#standardize the theta
standard_theta <- cov2cor(theta)
#calculate the inverse of theta
covMatrix <- solve(standard_theta)
#generate nod random samples from a multivariate gaussian distribution with zero mean and the covariance matrix sigma = theta^-1.
testdata <- mvrnorm(n = n, mu = numeric(nod), Sigma = covMatrix, tol = 0, empirical = FALSE, EISPACK = FALSE)
newList <- list("theta" = standard_theta, "testdata" = testdata)
return(newList)
}
sampleData <- simulation(p = 20, n = 5000)
trueTheta <- sampleData$theta
trueTheta
trainIndex <- sample(seq(nrow(sampleData$testdata)), floor(nrow(sampleData$testdata))/2, replace=FALSE)
train <- sampleData$testdata[trainIndex,]
test <- sampleData$testdata[-trainIndex,]
s <- var(train)  # Empirical Covariance Matrix
s
EBICglasso(s, n = 2500)
ebic <- EBICglasso(s, n = 2500)
ebic <- EBICglasso(s, n = 2500, returnAllResults = FALSE)
ebic
ebic <- EBICglasso(s, n = 2500, returnAllResults = TRUE)
View(ebic)
ebic$ebic
which.min(ebic$ebic)
ebic$lambda[which.min(ebic$ebic)]
optMatrixEBIC <- ebic$optnet
optMatrixEBIC
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
amsterdam <- read_csv('st445_final_data')
knitr::opts_chunk$set(echo = TRUE)
y <- amsterdam$logprice
suppressMessages(library(readr))
suppressMessages(library(glmnet))
amsterdam <- read_csv('st445_final_data')
amsterdam <- amsterdam[,-c(1,15)]
# glmnet does not use formula language
x <- model.matrix(logprice ~ ., data = amsterdam)
y <- amsterdam$logprice
plot(fit.ridge, xvar="lambda", label= TRUE)
fit.ridge <-glmnet(x, y, alpha=0)
plot(fit.ridge, xvar="lambda", label= TRUE)
plot(fit.ridge, xvar="dev", label= TRUE)
subIris <- iris[iris$Species == 'virginica',]
mean(subIris$Petal.Width)
hello <- function(firstname, lastname, title = 'Ms.'){
if (missing(lastname)){
return(cat('Hey, ', firstname,'!', sep=""))
} else {
return(cat('Hello,', title, firstname, lastname))
}
}
hello <- function(firstname, lastname, title = 'Ms.'){
if (missing(lastname)){
return(cat('Hey, ', firstname,'!', sep=""))
} else {
return(cat('Hello,', title, firstname, lastname))
}
}
hello("Christian", "Mueller", "Mr.")
wholesaleData <- read.csv('Wholesale customers data.csv')
wholesaleData <- select(wholesaleData, Fresh, Milk, Grocery, Frozen, Detergents_Paper, Delicassen)
# Outputs a tibble with the relevant data
suppressMessages(library(dplyr))
wholesaleData <- select(wholesaleData, Fresh, Milk, Grocery, Frozen, Detergents_Paper, Delicassen)
get_distance <- function(p1,p2){
sqrt(sum((p1 - p2) ^ 2))
}
get_centroid <- function(points) {
centroid <- vector(mode = "integer", ncol(points))
for(i in 1:ncol(points)){
centroid[i] <- mean(points[,i])
}
return(centroid)
}
kmeans <- function(data, noOfClus){
# Randomly sample k centroids
centroids <- sample_n(data, noOfClus)
# cluster is a vector of integers (from 1:k) indicating
# the cluster to which each point is allocated. Vector of 0's first.
cluster <- vector(mode = "integer", length = nrow(data))
# distance is overwritten and thus only defined like this once.
distance <- vector(mode = "integer", length = noOfClus)
# Count no of iterations, 0 first
iter <- 0
# Necessary for convergence condition
clusterToCompare <- c(rep(1, nrow(data)))
# while loop stops when all points remain in the same cluster
# two iterations in a row.
while (!identical(cluster,clusterToCompare)){
clusterToCompare <- cluster
# Fill the cluster vector
for(i in 1:nrow(data)){
for(j in 1:noOfClus){
distance[j] <- get_distance(data[i,],centroids[j,])
}
cluster[i] <- which.min(distance)
}
# Overwrite centroid vector
for(i in 1:noOfClus){
centroids[i,] <- get_centroid(data[cluster == i,])
}
iter <- iter + 1
}
returnList <- list('cluster' = cluster, 'iter' = iter)
return(returnList)
}
# Running the command takes about 30 seconds.
kmeansTry <- kmeans(wholesaleData, 3)
kmeansTry$cluster
kmeansTry$iter
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(readr))
suppressMessages(library(glmnet))
amsterdam <- read_csv('st445_final_data')
amsterdam <- amsterdam[,-c(1,15)]
# glmnet does not use formula language
x <- model.matrix(logprice ~ ., data = amsterdam)
y <- amsterdam$logprice
fit.ridge <-glmnet(x, y, alpha=0)
# 8, 7, 15, 20, 16 most important var
plot(fit.ridge, xvar="lambda", label= TRUE)
plot(fit.ridge, xvar="lambda", label= TRUE)
plot(fit.ridge, xvar="dev", label= TRUE)
View(x)
reg1 <- lm(logprice ~ ., data = amsterdam)
summary(reg1)
plot(fit.ridge, xvar="lambda", label= TRUE)
cv.ridge <- cv.glmnet(x, y, alpha=0)
cv.ridge <- cv.glmnet(x, y, alpha=0)
## Plot of CV mse vs log (lambda), small lambda is best apparently.
plot(cv.ridge)
plot(cv.ridge)
## Coefficent vector corresponding to the mse which is within one standard error of the lowest mse using the best lambda.
coef(cv.ridge)
## Coefficient vector corresponding to the lowest mse using the best lambda
coef(glmnet(x,y,alpha=0, lambda=cv.ridge$lambda.min))
?glmnet
fit.ridge <-glmnet(x, y, alpha=0, family = "guassian")
fit.ridge <-glmnet(x, y, alpha=0, family = "gaussian")
# 8, 7, 15, 20, 16 most important var
plot(fit.ridge, xvar="lambda", label= TRUE)
plot(fit.ridge, xvar="lambda", label= TRUE)
plot(fit.ridge, xvar="dev", label= TRUE)
cv.ridge <- cv.glmnet(x, y, alpha=0, family = "gaussian")
## Plot of CV mse vs log (lambda), small lambda is best apparently.
plot(cv.ridge)
plot(cv.ridge)
?glmnet
ridge.train <- cv.glmnet(x[train,], y[train], alpha = 0)
# finding MSE
traingsize = floor(0.7*nrow(amsterdam))
set.seed(123)
train = sample(seq_len(nrow(amsterdam)),size = traingsize)
ridge.train <- cv.glmnet(x[train,], y[train], alpha = 0)
pred.test.ridge <-predict(ridge.train, x[-train,])
dim(pred.test.ridge)
rmse.ridge <-sqrt(apply((y[-train]-pred.test.ridge)^2,2,mean))
plot(log(ridge.train$lambda), rmse.ridge, type="b", xlab="Log(lambda)")
lambda.best.ridge <- ridge.train$lambda[order(rmse.ridge)[1]]
lambda.best.ridge
mseRidge <- min(rmse.ridge)
mseRidge
rmse.ridge
ridge.train <- glmnet(x[train,], y[train], alpha = 0)
pred.test.ridge <-predict(ridge.train, x[-train,])
dim(pred.test.ridge)
rmse.ridge <-sqrt(apply((y[-train]-pred.test.ridge)^2,2,mean))
plot(log(ridge.train$lambda), rmse.ridge, type="b", xlab="Log(lambda)")
lambda.best.ridge <- ridge.train$lambda[order(rmse.ridge)[1]]
lambda.best.ridge
mseRidge <- min(rmse.ridge)
mseRidge
ridge.train <- glmnet(x[train,], y[train], alpha = 0, nlambda = 1000)
pred.test.ridge <-predict(ridge.train, x[-train,])
rmse.ridge <-sqrt(apply((y[-train]-pred.test.ridge)^2,2,mean))
plot(log(ridge.train$lambda), rmse.ridge, type="b", xlab="Log(lambda)")
lambda.best.ridge <- ridge.train$lambda[order(rmse.ridge)[1]]
lambda.best.ridge
mseRidge <- min(rmse.ridge)
mseRidge
plot(log(ridge.train$lambda), rmse.ridge, type="b", xlab="Log(lambda)")
ridge.train <- glmnet(x[train,], y[train], alpha = 0, nlambda = 1000, lambda.min.ratio = 0.1)
pred.test.ridge <-predict(ridge.train, x[-train,])
rmse.ridge <-sqrt(apply((y[-train]-pred.test.ridge)^2,2,mean))
rmse.ridge <-sqrt(apply((y[-train]-pred.test.ridge)^2,2,mean))
plot(log(ridge.train$lambda), rmse.ridge, type="b", xlab="Log(lambda)")
lambda.best.ridge <- ridge.train$lambda[order(rmse.ridge)[1]]
lambda.best.ridge
mseRidge <- min(rmse.ridge)
mseRidge
ridge.train <- glmnet(x[train,], y[train], alpha = 0, nlambda = 500)
pred.test.ridge <-predict(ridge.train, x[-train,])
rmse.ridge <-sqrt(apply((y[-train]-pred.test.ridge)^2,2,mean))
rmse.ridge <-sqrt(apply((y[-train]-pred.test.ridge)^2,2,mean))
plot(log(ridge.train$lambda), rmse.ridge, type="b", xlab="Log(lambda)")
plot(log(ridge.train$lambda), rmse.ridge, type="b", xlab="Log(lambda)")
lambda.best.ridge <- ridge.train$lambda[order(rmse.ridge)[1]]
lambda.best.ridge
mseRidge <- min(rmse.ridge)
mseRidge
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar="lambda", label= TRUE)
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar="lambda", label= TRUE)
plot(fit.lasso, xvar="dev", label= TRUE)
cv.lasso <-cv.glmnet(x, y)
plot(cv.lasso)
# Use very small lambda, again
## coefficent vector corresponding to the mse which is within one standard error of the lowest mse using the best lambda.
coef(cv.lasso)
## coefficient vector corresponding to the lowest mse using the best lambda
coef(glmnet(x,y, lambda=cv.lasso$lambda.min))
## Validation set approach to select best lambda in Lasso
lasso.train <-glmnet(x[train,], y[train])
pred.test <-predict(lasso.train, x[-train,])
dim(pred.test)
rmse <-sqrt(apply((y[-train]-pred.test)^2,2,mean))
plot(log(lasso.train$lambda), rmse, type="b", xlab="Log(lambda)")
lambda.best <-lasso.train$lambda[order(rmse)[1]]
lambda.best
mseLasso <- min(rmse)
mseLasso
rmse
knitr::opts_chunk$set(echo = TRUE)
lambda.best.ridge
rm(list = ls())
setwd("~/Documents/Rworkspace/ST443")
library(readr)
setwd("~/Documents/LSE/ST443/Project/ST443-Group-project/1-XGBoost")
rm(list = ls())
library(readr)
library(dplyr)
library(xgboost)
library(stringr)
library(caret)
library(car)
library(fastDummies)
library(ModelMetrics)
amsterdam <- read_csv('st443_final_data')
amsterdam <- amsterdam[,-c(1,15)]
amsterdam <- mutate(amsterdam,
instant_bookable = ifelse(instant_bookable == TRUE, 1, 0))
#output_vector = amsterdam[,'logprice']
amsterdam <- fastDummies::dummy_cols(amsterdam)
amsterdam <- amsterdam[,-c(5,10,13,16,22,26)]
View(amsterdam)
traingsize = floor(0.7*nrow(amsterdam))
set.seed(123)
trainindex = sample(seq_len(nrow(amsterdam)),size = traingsize)
train_df <- amsterdam[trainindex,]
test_df <- amsterdam[-trainindex,]
train <- as.matrix(train_df, rownames.force = NA)
test <- as.matrix(test_df, rownames.force = NA)
train <- as(train, "sparseMatrix")
test <- as(test, "sparseMatrix")
train_data <- xgb.DMatrix(data = train[,-12], label = train[,"logprice"])
train_df
train_df["logprice"]
train_df[="logprice"]
train_df[-"logprice"]
View(train_df)
train_df[-12]
xgb_grid = expand.grid(
nrounds = 1000,
eta = c(0.1, 0.05, 0.01),
max_depth = c(2, 3, 4, 5, 6),
gamma = 0,
colsample_bytree=1,
min_child_weight=c(1, 2, 3, 4 ,5),
subsample=1
)
my_control <-trainControl(method="cv", number=5)
xgb_caret <- train(x = train_df[-12], y = train_df["logprice"],
method='xgbTree', trControl= my_control,
tuneGrid = xgb_grid)
xgb_caret <- train(x = train_df[-12], y = train_df$logprice,
method='xgbTree', trControl= my_control,
tuneGrid = xgb_grid)
xgb_caret$bestTune
default_param <- list(
objective = "reg:linear",
booster = "gbtree",
eta=0.01, #default = 0.3
gamma=0,
max_depth=5, #default=6
min_child_weight=1, #default=1
subsample=1,
colsample_bytree=1
)
rm(list = ls())
library(readr)
library(dplyr)
library(xgboost)
library(stringr)
library(caret)
library(car)
library(fastDummies)
library(ModelMetrics)
amsterdam <- read_csv('st443_final_data')
amsterdam <- amsterdam[,-c(1,15)]
amsterdam <- mutate(amsterdam,
instant_bookable = ifelse(instant_bookable == TRUE, 1, 0))
amsterdam <- fastDummies::dummy_cols(amsterdam)
amsterdam <- amsterdam[,-c(5,10,13,16,22,26)]
traingsize = floor(0.7*nrow(amsterdam))
set.seed(123)
trainindex = sample(seq_len(nrow(amsterdam)),size = traingsize)
train_df <- amsterdam[trainindex,]
test_df <- amsterdam[-trainindex,]
trainmatrix <- as.matrix(train_df, rownames.force = NA)
testmatrix <- as.matrix(test_df, rownames.force = NA)
dtrain <- as(trainmatrix, "sparseMatrix")
dtest <- as(testmatrix, "sparseMatrix")
train_data <- xgb.DMatrix(data = train[,-12], label = train[,"logprice"])
test_data <- xgb.DMatrix(data = test[,-12])
rm(list = ls())
library(readr)
library(dplyr)
library(xgboost)
library(stringr)
library(caret)
library(car)
library(fastDummies)
library(ModelMetrics)
amsterdam <- read_csv('st443_final_data')
amsterdam <- amsterdam[,-c(1,15)]
amsterdam <- mutate(amsterdam,
instant_bookable = ifelse(instant_bookable == TRUE, 1, 0))
amsterdam <- fastDummies::dummy_cols(amsterdam)
amsterdam <- amsterdam[,-c(5,10,13,16,22,26)]
traingsize = floor(0.7*nrow(amsterdam))
set.seed(123)
trainindex = sample(seq_len(nrow(amsterdam)),size = traingsize)
train_df <- amsterdam[trainindex,]
test_df <- amsterdam[-trainindex,]
trainmatrix <- as.matrix(train_df, rownames.force = NA)
testmatrix <- as.matrix(test_df, rownames.force = NA)
dtrain <- as(trainmatrix, "sparseMatrix")
dtest <- as(testmatrix, "sparseMatrix")
train_data <- xgb.DMatrix(data = dtrain[,-12], label = train[,"logprice"])
test_data <- xgb.DMatrix(data = dtest[,-12])
rm(list = ls())
library(readr)
library(dplyr)
library(xgboost)
library(stringr)
library(caret)
library(car)
library(fastDummies)
library(ModelMetrics)
amsterdam <- read_csv('st443_final_data')
amsterdam <- amsterdam[,-c(1,15)]
amsterdam <- mutate(amsterdam,
instant_bookable = ifelse(instant_bookable == TRUE, 1, 0))
amsterdam <- fastDummies::dummy_cols(amsterdam)
amsterdam <- amsterdam[,-c(5,10,13,16,22,26)]
traingsize = floor(0.7*nrow(amsterdam))
set.seed(123)
trainindex = sample(seq_len(nrow(amsterdam)),size = traingsize)
train_df <- amsterdam[trainindex,]
test_df <- amsterdam[-trainindex,]
trainmatrix <- as.matrix(train_df, rownames.force = NA)
testmatrix <- as.matrix(test_df, rownames.force = NA)
dtrain <- as(trainmatrix, "sparseMatrix")
dtest <- as(testmatrix, "sparseMatrix")
train_data <- xgb.DMatrix(data = dtrain[,-12], label = dtrain$logprice)
test_data <- xgb.DMatrix(data = dtest[,-12])
rm(list = ls())
library(readr)
library(dplyr)
library(xgboost)
library(stringr)
library(caret)
library(car)
library(fastDummies)
library(ModelMetrics)
amsterdam <- read_csv('st443_final_data')
amsterdam <- amsterdam[,-c(1,15)]
amsterdam <- mutate(amsterdam,
instant_bookable = ifelse(instant_bookable == TRUE, 1, 0))
amsterdam <- fastDummies::dummy_cols(amsterdam)
amsterdam <- amsterdam[,-c(5,10,13,16,22,26)]
traingsize = floor(0.7*nrow(amsterdam))
set.seed(123)
trainindex = sample(seq_len(nrow(amsterdam)),size = traingsize)
train_df <- amsterdam[trainindex,]
test_df <- amsterdam[-trainindex,]
trainmatrix <- as.matrix(train_df, rownames.force = NA)
testmatrix <- as.matrix(test_df, rownames.force = NA)
dtrain <- as(trainmatrix, "sparseMatrix")
dtest <- as(testmatrix, "sparseMatrix")
train_data <- xgb.DMatrix(data = dtrain[,-12], label = dtrain["logprice"])
test_data <- xgb.DMatrix(data = dtest[,-12])
rm(list = ls())
library(readr)
library(dplyr)
library(xgboost)
library(stringr)
library(caret)
library(car)
library(fastDummies)
library(ModelMetrics)
amsterdam <- read_csv('st443_final_data')
amsterdam <- amsterdam[,-c(1,15)]
amsterdam <- mutate(amsterdam,
instant_bookable = ifelse(instant_bookable == TRUE, 1, 0))
amsterdam <- fastDummies::dummy_cols(amsterdam)
amsterdam <- amsterdam[,-c(5,10,13,16,22,26)]
traingsize = floor(0.7*nrow(amsterdam))
set.seed(123)
trainindex = sample(seq_len(nrow(amsterdam)),size = traingsize)
train_df <- amsterdam[trainindex,]
test_df <- amsterdam[-trainindex,]
trainmatrix <- as.matrix(train_df, rownames.force = NA)
testmatrix <- as.matrix(test_df, rownames.force = NA)
dtrain <- as(trainmatrix, "sparseMatrix")
dtest <- as(testmatrix, "sparseMatrix")
train_data <- xgb.DMatrix(data = dtrain[,-12], label = dtrain[,"logprice"])
test_data <- xgb.DMatrix(data = dtest[,-12])
default_param <- list(
objective = "reg:linear",
booster = "gbtree",
eta=0.01, #default = 0.3
gamma=0,
max_depth=5, #default=6
min_child_weight=1, #default=1
subsample=1,
colsample_bytree=1
)
xgbcv <- xgb.cv( params = default_param,
data = dtrain, nrounds = 500,
nfold = 5,
showsd = T,
stratified = T,
print_every_n = 40,
early_stopping_rounds = 10,
maximize = F)
xgbcv <- xgb.cv( params = default_param,
data = dtrain, nrounds = 1000,
nfold = 5,
showsd = T,
stratified = T,
print_every_n = 40,
early_stopping_rounds = 10,
maximize = F,
label = dtrain[,"logprice"])
xgb_mod <- xgb.train(data = dtrain, params = default_param, nrounds = 1000)
xgb_mod <- xgb.train(data = train_data, params = default_param, nrounds = 1000)
XGBpred <- predict(xgb_mod, dtest)
XGBpred <- predict(xgb_mod, test_data)
rmse(test_df$logprice,XGBpred)
